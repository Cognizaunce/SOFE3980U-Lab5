{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFz7-vlkzBF"
      },
      "source": [
        "# CleanLab Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FslSnc5HyvwV"
      },
      "outputs": [],
      "source": [
        "!pip install \"cleanlab[datalab]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_k1Zbeus_l78"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from cleanlab.classification import CleanLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DYm3OMMYcNRY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n"
          ]
        }
      ],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lnNneeE8cZOi"
      },
      "outputs": [],
      "source": [
        "# Introduce anomalies by altering some values\n",
        "np.random.seed(42)\n",
        "anomaly_indices = np.random.choice(df.index, size=10, replace=False)\n",
        "df.loc[anomaly_indices, 'petal length (cm)'] = np.random.uniform(5, 7, size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-nkO_L78caDI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anomalies detected at indices: [ 18  31  68  82 106 119]\n",
            "Suspected anomaly values: [[5.7        3.8        5.82076585 0.3       ]\n",
            " [5.4        3.4        5.57950291 0.4       ]\n",
            " [6.2        2.2        6.61624076 1.5       ]\n",
            " [5.8        2.7        6.26680751 1.2       ]\n",
            " [4.9        2.5        4.5        1.7       ]\n",
            " [6.         2.2        5.         1.5       ]]\n"
          ]
        }
      ],
      "source": [
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Use CleanLearning for anomaly detection\n",
        "clf = CleanLearning()\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Find potential anomalies in labels\n",
        "label_issues = clf.find_label_issues(X, y)\n",
        "\n",
        "# Output the anomalies\n",
        "anomalies = np.where(label_issues[\"is_label_issue\"])[0]\n",
        "print(f\"Anomalies detected at indices: {anomalies}\")\n",
        "print(f\"Suspected anomaly values: {X[anomalies]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RezXah9nc5eG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                      Suspected Anomalous Data Points\n",
            "-----------------------------------------------------------------------------------------------------------\n",
            " Index  sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  True Label Flower Species\n",
            "    18                5.7               3.8           5.820766               0.3         0.0         Setosa\n",
            "    31                5.4               3.4           5.579503               0.4         0.0         Setosa\n",
            "    68                6.2               2.2           6.616241               1.5         1.0     Versicolor\n",
            "    82                5.8               2.7           6.266808               1.2         1.0     Versicolor\n",
            "   106                4.9               2.5           4.500000               1.7         2.0      Virginica\n",
            "   119                6.0               2.2           5.000000               1.5         2.0      Virginica\n"
          ]
        }
      ],
      "source": [
        "# Create an empty list to store DataFrames\n",
        "suspect_dfs = []\n",
        "\n",
        "flower_species = {0.0: \"Setosa\", 1.0: \"Versicolor\", 2.0: \"Virginica\"}\n",
        "\n",
        "# Loop over the indices and create a structured DataFrame for each\n",
        "for idx in anomalies:\n",
        "    # Create a DataFrame for the suspected anomaly data point\n",
        "    df_suspect = pd.DataFrame([df.iloc[idx][iris.feature_names].values], columns=iris.feature_names)\n",
        "    df_suspect.insert(0, \"Index\", idx)  # Insert index column\n",
        "\n",
        "    df_suspect[\"True Label\"] = df.iloc[idx][\"target\"]\n",
        "    df_suspect[\"Flower Species\"] = flower_species[y[idx]]  # Map label to flower species\n",
        "\n",
        "    # Append the current suspect DataFrame to the list\n",
        "    suspect_dfs.append(df_suspect)\n",
        "\n",
        "# Combine all the suspect DataFrames into a single DataFrame\n",
        "df_all_suspects = pd.concat(suspect_dfs, ignore_index=True)\n",
        "\n",
        "# Print the full table of suspected anomalies\n",
        "print(\"\\n                                      Suspected Anomalous Data Points\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(df_all_suspects.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
